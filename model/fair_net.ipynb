{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Activation, Dense, Flatten, Conv2D, MaxPooling2D, \n",
    "    GlobalAveragePooling2D, AveragePooling2D, BatchNormalization, add , Dropout )\n",
    "import tensorflow.keras.regularizers as regulizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvWithBatchNorm(tf.keras.layers.Conv2D):\n",
    "    \n",
    "    def __init__( self, activation = 'relu' , name = 'conbn' , **kwargs ):\n",
    "        super().__init__(activation=None , name = name  , **kwargs)\n",
    "        self.activation = Activation(activation , name = name +'_bn') if activation is not None else None\n",
    "        self.batch_norm = BatchNormalization(axis = -1 , name = name + '_bn')\n",
    "        \n",
    "    def call(self, inputs , training = None ):\n",
    "        x = super().call(inputs)\n",
    "        x = self.batch_norm( x , training = training )\n",
    "        if self.activation is not None:\n",
    "            x = self.activation(x)  \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(tf.keras.models.Sequential):\n",
    "    \n",
    "    def __init__(self , inputs_shape , name = 'fire_net' ):\n",
    "        self.number_class = 2\n",
    "        self.filters = 64\n",
    "        self.inputs_shape = inputs_shape\n",
    "        self.strides = 1\n",
    "        \n",
    "        sequential_lst = []\n",
    "        sequential_lst.append(Input(shape = self.inputs_shape , name = 'input'))\n",
    "        \n",
    "        sequential_lst.append(ConvWithBatchNorm(filters = 16 , kernel_size = 3 , strides = 1 , padding = 'same', activation='relu',name = 'block_1'))\n",
    "        sequential_lst.append(AveragePooling2D())\n",
    "        sequential_lst.append(Dropout(0.5))\n",
    "        \n",
    "        \n",
    "        sequential_lst.append(ConvWithBatchNorm(filters = 32 , kernel_size = 3 , strides = 1 , padding = 'valid', activation='relu',name = 'block_2'))\n",
    "        sequential_lst.append(AveragePooling2D())\n",
    "        sequential_lst.append(Dropout(0.5))\n",
    "        \n",
    "        \n",
    "        sequential_lst.append(ConvWithBatchNorm(filters = 64 , kernel_size = 3 , strides = 1 , padding = 'valid', activation='relu',name = 'block_3'))\n",
    "        sequential_lst.append(AveragePooling2D())\n",
    "        sequential_lst.append(Dropout(0.5))\n",
    "        \n",
    "        \n",
    "        sequential_lst.append(Flatten())\n",
    "        sequential_lst.append(Dense(units=256, activation='relu'))\n",
    "        sequential_lst.append(Dropout(0.2))\n",
    "        \n",
    "        sequential_lst.append(Dense(units=128, activation='relu'))\n",
    "        sequential_lst.append(Dense(units=self.number_class, activation = 'softmax'))\n",
    "        \n",
    "        \n",
    "        super().__init__(sequential_lst , name = name )\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297AC5D148>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297AC5D148>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297AC5D148>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297AC5D148>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297AC63B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297AC63B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297AC63B88>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297AC63B88>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING:tensorflow:Entity <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297ACB5A08>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297ACB5A08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
      "WARNING: Entity <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297ACB5A08>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method ConvWithBatchNorm.call of <__main__.ConvWithBatchNorm object at 0x000002297ACB5A08>>: AssertionError: Bad argument number for Name: 3, expecting 4\n"
     ]
    }
   ],
   "source": [
    "input_shape = [64, 64, 3]\n",
    "model = Model(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"fire_net\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "block_1 (ConvWithBatchNorm)  (None, 64, 64, 16)        512       \n",
      "_________________________________________________________________\n",
      "average_pooling2d (AveragePo (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "block_2 (ConvWithBatchNorm)  (None, 30, 30, 32)        4768      \n",
      "_________________________________________________________________\n",
      "average_pooling2d_1 (Average (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "block_3 (ConvWithBatchNorm)  (None, 13, 13, 64)        18752     \n",
      "_________________________________________________________________\n",
      "average_pooling2d_2 (Average (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               590080    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 647,266\n",
      "Trainable params: 647,042\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATADIR = 'D:/project/fire_net/Dataset/'\n",
    "CATEGORIES = ['Fire', 'NoFire']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data():\n",
    "    traing_data = []\n",
    "    \n",
    "    for category in CATEGORIES:\n",
    "        path = os.path.join(DATADIR , category)\n",
    "        class_num = CATEGORIES.index(category)\n",
    "        \n",
    "        for img in tqdm(os.listdir(path)):\n",
    "            try:\n",
    "                image = cv2.imread(os.path.join(path, img))\n",
    "                image = cv2.resize(image , (64 , 64 ))\n",
    "                traing_data.append([image , class_num])\n",
    "            except Exception as e :\n",
    "                print(e)\n",
    "            \n",
    "    return traing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                         | 0/1124 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████| 1124/1124 [00:09<00:00, 117.53it/s]\n",
      " 68%|█████████████████████████████████████████████████████▍                         | 880/1301 [00:14<00:13, 31.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV(4.1.2) C:\\projects\\opencv-python\\opencv\\modules\\imgproc\\src\\resize.cpp:3720: error: (-215:Assertion failed) !ssize.empty() in function 'cv::resize'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 1301/1301 [00:21<00:00, 61.25it/s]\n"
     ]
    }
   ],
   "source": [
    "training_dataset = create_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "for img , label in training_dataset:\n",
    "    X.append(img)\n",
    "    Y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X).reshape(-1 , 64 , 64 , 3 )\n",
    "X = X/255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2423, 64, 64, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1696 samples, validate on 727 samples\n",
      "Epoch 1/5\n",
      "1696/1696 [==============================] - ETA: 11s - loss: 0.1393 - accuracy: 0.937 - ETA: 10s - loss: 0.1452 - accuracy: 0.921 - ETA: 10s - loss: 0.1503 - accuracy: 0.927 - ETA: 10s - loss: 0.1418 - accuracy: 0.929 - ETA: 10s - loss: 0.1849 - accuracy: 0.925 - ETA: 10s - loss: 0.1675 - accuracy: 0.932 - ETA: 9s - loss: 0.1858 - accuracy: 0.928 - ETA: 9s - loss: 0.1966 - accuracy: 0.92 - ETA: 9s - loss: 0.1893 - accuracy: 0.92 - ETA: 9s - loss: 0.1936 - accuracy: 0.93 - ETA: 9s - loss: 0.1820 - accuracy: 0.93 - ETA: 8s - loss: 0.1702 - accuracy: 0.94 - ETA: 8s - loss: 0.1635 - accuracy: 0.94 - ETA: 8s - loss: 0.1561 - accuracy: 0.94 - ETA: 8s - loss: 0.1567 - accuracy: 0.94 - ETA: 7s - loss: 0.1493 - accuracy: 0.94 - ETA: 7s - loss: 0.1543 - accuracy: 0.94 - ETA: 7s - loss: 0.1563 - accuracy: 0.94 - ETA: 7s - loss: 0.1567 - accuracy: 0.94 - ETA: 7s - loss: 0.1546 - accuracy: 0.94 - ETA: 6s - loss: 0.1519 - accuracy: 0.94 - ETA: 6s - loss: 0.1485 - accuracy: 0.94 - ETA: 6s - loss: 0.1442 - accuracy: 0.94 - ETA: 6s - loss: 0.1407 - accuracy: 0.94 - ETA: 5s - loss: 0.1475 - accuracy: 0.94 - ETA: 5s - loss: 0.1497 - accuracy: 0.94 - ETA: 5s - loss: 0.1508 - accuracy: 0.94 - ETA: 5s - loss: 0.1500 - accuracy: 0.94 - ETA: 5s - loss: 0.1507 - accuracy: 0.94 - ETA: 4s - loss: 0.1548 - accuracy: 0.94 - ETA: 4s - loss: 0.1534 - accuracy: 0.94 - ETA: 4s - loss: 0.1550 - accuracy: 0.93 - ETA: 4s - loss: 0.1541 - accuracy: 0.93 - ETA: 4s - loss: 0.1544 - accuracy: 0.93 - ETA: 3s - loss: 0.1565 - accuracy: 0.93 - ETA: 3s - loss: 0.1551 - accuracy: 0.93 - ETA: 3s - loss: 0.1538 - accuracy: 0.94 - ETA: 3s - loss: 0.1535 - accuracy: 0.94 - ETA: 2s - loss: 0.1525 - accuracy: 0.93 - ETA: 2s - loss: 0.1540 - accuracy: 0.93 - ETA: 2s - loss: 0.1544 - accuracy: 0.93 - ETA: 2s - loss: 0.1564 - accuracy: 0.93 - ETA: 2s - loss: 0.1569 - accuracy: 0.93 - ETA: 1s - loss: 0.1561 - accuracy: 0.93 - ETA: 1s - loss: 0.1556 - accuracy: 0.93 - ETA: 1s - loss: 0.1574 - accuracy: 0.93 - ETA: 1s - loss: 0.1573 - accuracy: 0.93 - ETA: 1s - loss: 0.1550 - accuracy: 0.93 - ETA: 0s - loss: 0.1537 - accuracy: 0.93 - ETA: 0s - loss: 0.1522 - accuracy: 0.94 - ETA: 0s - loss: 0.1515 - accuracy: 0.94 - ETA: 0s - loss: 0.1527 - accuracy: 0.94 - 12s 7ms/sample - loss: 0.1534 - accuracy: 0.9399 - val_loss: 0.2244 - val_accuracy: 0.9175\n",
      "Epoch 2/5\n",
      "1696/1696 [==============================] - ETA: 10s - loss: 0.2253 - accuracy: 0.906 - ETA: 10s - loss: 0.1852 - accuracy: 0.921 - ETA: 10s - loss: 0.1614 - accuracy: 0.937 - ETA: 10s - loss: 0.1595 - accuracy: 0.937 - ETA: 10s - loss: 0.1576 - accuracy: 0.937 - ETA: 9s - loss: 0.1452 - accuracy: 0.942 - ETA: 9s - loss: 0.1340 - accuracy: 0.94 - ETA: 9s - loss: 0.1270 - accuracy: 0.95 - ETA: 9s - loss: 0.1224 - accuracy: 0.95 - ETA: 9s - loss: 0.1299 - accuracy: 0.95 - ETA: 8s - loss: 0.1227 - accuracy: 0.95 - ETA: 8s - loss: 0.1253 - accuracy: 0.95 - ETA: 8s - loss: 0.1223 - accuracy: 0.95 - ETA: 8s - loss: 0.1223 - accuracy: 0.95 - ETA: 8s - loss: 0.1157 - accuracy: 0.96 - ETA: 7s - loss: 0.1205 - accuracy: 0.95 - ETA: 7s - loss: 0.1257 - accuracy: 0.95 - ETA: 7s - loss: 0.1248 - accuracy: 0.95 - ETA: 7s - loss: 0.1220 - accuracy: 0.95 - ETA: 6s - loss: 0.1173 - accuracy: 0.95 - ETA: 6s - loss: 0.1169 - accuracy: 0.95 - ETA: 6s - loss: 0.1236 - accuracy: 0.95 - ETA: 6s - loss: 0.1196 - accuracy: 0.95 - ETA: 6s - loss: 0.1254 - accuracy: 0.95 - ETA: 5s - loss: 0.1258 - accuracy: 0.95 - ETA: 5s - loss: 0.1247 - accuracy: 0.95 - ETA: 5s - loss: 0.1290 - accuracy: 0.95 - ETA: 5s - loss: 0.1280 - accuracy: 0.95 - ETA: 5s - loss: 0.1358 - accuracy: 0.95 - ETA: 4s - loss: 0.1383 - accuracy: 0.95 - ETA: 4s - loss: 0.1363 - accuracy: 0.95 - ETA: 4s - loss: 0.1410 - accuracy: 0.94 - ETA: 4s - loss: 0.1422 - accuracy: 0.94 - ETA: 4s - loss: 0.1443 - accuracy: 0.94 - ETA: 3s - loss: 0.1454 - accuracy: 0.94 - ETA: 3s - loss: 0.1427 - accuracy: 0.94 - ETA: 3s - loss: 0.1419 - accuracy: 0.94 - ETA: 3s - loss: 0.1426 - accuracy: 0.94 - ETA: 2s - loss: 0.1416 - accuracy: 0.94 - ETA: 2s - loss: 0.1396 - accuracy: 0.95 - ETA: 2s - loss: 0.1408 - accuracy: 0.94 - ETA: 2s - loss: 0.1433 - accuracy: 0.94 - ETA: 2s - loss: 0.1417 - accuracy: 0.94 - ETA: 1s - loss: 0.1397 - accuracy: 0.94 - ETA: 1s - loss: 0.1376 - accuracy: 0.95 - ETA: 1s - loss: 0.1394 - accuracy: 0.95 - ETA: 1s - loss: 0.1382 - accuracy: 0.95 - ETA: 1s - loss: 0.1360 - accuracy: 0.95 - ETA: 0s - loss: 0.1379 - accuracy: 0.95 - ETA: 0s - loss: 0.1401 - accuracy: 0.95 - ETA: 0s - loss: 0.1389 - accuracy: 0.95 - ETA: 0s - loss: 0.1412 - accuracy: 0.95 - 12s 7ms/sample - loss: 0.1406 - accuracy: 0.9499 - val_loss: 0.1849 - val_accuracy: 0.9285\n",
      "Epoch 3/5\n",
      "1696/1696 [==============================] - ETA: 10s - loss: 0.0896 - accuracy: 0.968 - ETA: 10s - loss: 0.1039 - accuracy: 0.953 - ETA: 10s - loss: 0.0903 - accuracy: 0.958 - ETA: 10s - loss: 0.1126 - accuracy: 0.953 - ETA: 10s - loss: 0.1141 - accuracy: 0.956 - ETA: 10s - loss: 0.1191 - accuracy: 0.953 - ETA: 10s - loss: 0.1118 - accuracy: 0.955 - ETA: 9s - loss: 0.0999 - accuracy: 0.960 - ETA: 9s - loss: 0.1036 - accuracy: 0.95 - ETA: 9s - loss: 0.0970 - accuracy: 0.95 - ETA: 9s - loss: 0.0995 - accuracy: 0.95 - ETA: 8s - loss: 0.1074 - accuracy: 0.95 - ETA: 8s - loss: 0.1057 - accuracy: 0.95 - ETA: 8s - loss: 0.1115 - accuracy: 0.95 - ETA: 8s - loss: 0.1059 - accuracy: 0.95 - ETA: 8s - loss: 0.1063 - accuracy: 0.95 - ETA: 7s - loss: 0.1110 - accuracy: 0.95 - ETA: 7s - loss: 0.1076 - accuracy: 0.95 - ETA: 7s - loss: 0.1115 - accuracy: 0.95 - ETA: 7s - loss: 0.1081 - accuracy: 0.95 - ETA: 6s - loss: 0.1055 - accuracy: 0.95 - ETA: 6s - loss: 0.1090 - accuracy: 0.95 - ETA: 6s - loss: 0.1083 - accuracy: 0.95 - ETA: 6s - loss: 0.1086 - accuracy: 0.95 - ETA: 5s - loss: 0.1073 - accuracy: 0.95 - ETA: 5s - loss: 0.1057 - accuracy: 0.95 - ETA: 5s - loss: 0.1063 - accuracy: 0.95 - ETA: 5s - loss: 0.1042 - accuracy: 0.95 - ETA: 5s - loss: 0.1057 - accuracy: 0.95 - ETA: 4s - loss: 0.1059 - accuracy: 0.95 - ETA: 4s - loss: 0.1040 - accuracy: 0.95 - ETA: 4s - loss: 0.1042 - accuracy: 0.95 - ETA: 4s - loss: 0.1074 - accuracy: 0.95 - ETA: 4s - loss: 0.1137 - accuracy: 0.95 - ETA: 3s - loss: 0.1142 - accuracy: 0.95 - ETA: 3s - loss: 0.1144 - accuracy: 0.95 - ETA: 3s - loss: 0.1136 - accuracy: 0.95 - ETA: 3s - loss: 0.1139 - accuracy: 0.95 - ETA: 2s - loss: 0.1136 - accuracy: 0.95 - ETA: 2s - loss: 0.1125 - accuracy: 0.95 - ETA: 2s - loss: 0.1140 - accuracy: 0.95 - ETA: 2s - loss: 0.1126 - accuracy: 0.95 - ETA: 2s - loss: 0.1140 - accuracy: 0.95 - ETA: 1s - loss: 0.1149 - accuracy: 0.95 - ETA: 1s - loss: 0.1135 - accuracy: 0.95 - ETA: 1s - loss: 0.1119 - accuracy: 0.95 - ETA: 1s - loss: 0.1118 - accuracy: 0.95 - ETA: 1s - loss: 0.1097 - accuracy: 0.95 - ETA: 0s - loss: 0.1110 - accuracy: 0.95 - ETA: 0s - loss: 0.1110 - accuracy: 0.95 - ETA: 0s - loss: 0.1104 - accuracy: 0.95 - ETA: 0s - loss: 0.1107 - accuracy: 0.95 - 12s 7ms/sample - loss: 0.1111 - accuracy: 0.9570 - val_loss: 0.2251 - val_accuracy: 0.9422\n",
      "Epoch 4/5\n",
      "1696/1696 [==============================] - ETA: 10s - loss: 0.2483 - accuracy: 0.906 - ETA: 10s - loss: 0.1654 - accuracy: 0.937 - ETA: 10s - loss: 0.1578 - accuracy: 0.927 - ETA: 10s - loss: 0.1373 - accuracy: 0.945 - ETA: 10s - loss: 0.1384 - accuracy: 0.943 - ETA: 10s - loss: 0.1217 - accuracy: 0.947 - ETA: 10s - loss: 0.1360 - accuracy: 0.937 - ETA: 9s - loss: 0.1525 - accuracy: 0.933 - ETA: 9s - loss: 0.1554 - accuracy: 0.93 - ETA: 9s - loss: 0.1439 - accuracy: 0.94 - ETA: 9s - loss: 0.1422 - accuracy: 0.94 - ETA: 9s - loss: 0.1424 - accuracy: 0.94 - ETA: 8s - loss: 0.1383 - accuracy: 0.94 - ETA: 8s - loss: 0.1344 - accuracy: 0.94 - ETA: 8s - loss: 0.1353 - accuracy: 0.94 - ETA: 8s - loss: 0.1431 - accuracy: 0.93 - ETA: 7s - loss: 0.1390 - accuracy: 0.94 - ETA: 7s - loss: 0.1383 - accuracy: 0.94 - ETA: 7s - loss: 0.1325 - accuracy: 0.94 - ETA: 7s - loss: 0.1349 - accuracy: 0.94 - ETA: 6s - loss: 0.1353 - accuracy: 0.94 - ETA: 6s - loss: 0.1331 - accuracy: 0.94 - ETA: 6s - loss: 0.1323 - accuracy: 0.94 - ETA: 6s - loss: 0.1337 - accuracy: 0.94 - ETA: 6s - loss: 0.1340 - accuracy: 0.94 - ETA: 5s - loss: 0.1329 - accuracy: 0.94 - ETA: 5s - loss: 0.1370 - accuracy: 0.94 - ETA: 5s - loss: 0.1364 - accuracy: 0.94 - ETA: 5s - loss: 0.1406 - accuracy: 0.94 - ETA: 5s - loss: 0.1382 - accuracy: 0.94 - ETA: 4s - loss: 0.1391 - accuracy: 0.94 - ETA: 4s - loss: 0.1390 - accuracy: 0.94 - ETA: 4s - loss: 0.1362 - accuracy: 0.94 - ETA: 4s - loss: 0.1376 - accuracy: 0.94 - ETA: 3s - loss: 0.1372 - accuracy: 0.94 - ETA: 3s - loss: 0.1365 - accuracy: 0.94 - ETA: 3s - loss: 0.1341 - accuracy: 0.94 - ETA: 3s - loss: 0.1358 - accuracy: 0.94 - ETA: 3s - loss: 0.1364 - accuracy: 0.94 - ETA: 2s - loss: 0.1358 - accuracy: 0.94 - ETA: 2s - loss: 0.1362 - accuracy: 0.94 - ETA: 2s - loss: 0.1349 - accuracy: 0.94 - ETA: 2s - loss: 0.1353 - accuracy: 0.94 - ETA: 1s - loss: 0.1335 - accuracy: 0.94 - ETA: 1s - loss: 0.1362 - accuracy: 0.94 - ETA: 1s - loss: 0.1384 - accuracy: 0.94 - ETA: 1s - loss: 0.1379 - accuracy: 0.94 - ETA: 1s - loss: 0.1372 - accuracy: 0.94 - ETA: 0s - loss: 0.1362 - accuracy: 0.94 - ETA: 0s - loss: 0.1338 - accuracy: 0.94 - ETA: 0s - loss: 0.1341 - accuracy: 0.94 - ETA: 0s - loss: 0.1320 - accuracy: 0.94 - 12s 7ms/sample - loss: 0.1327 - accuracy: 0.9440 - val_loss: 0.1914 - val_accuracy: 0.9257\n",
      "Epoch 5/5\n",
      "1696/1696 [==============================] - ETA: 10s - loss: 0.1121 - accuracy: 0.937 - ETA: 10s - loss: 0.2366 - accuracy: 0.906 - ETA: 10s - loss: 0.1925 - accuracy: 0.937 - ETA: 10s - loss: 0.1701 - accuracy: 0.945 - ETA: 9s - loss: 0.1408 - accuracy: 0.956 - ETA: 9s - loss: 0.1291 - accuracy: 0.95 - ETA: 9s - loss: 0.1287 - accuracy: 0.95 - ETA: 9s - loss: 0.1255 - accuracy: 0.95 - ETA: 9s - loss: 0.1254 - accuracy: 0.95 - ETA: 8s - loss: 0.1192 - accuracy: 0.95 - ETA: 8s - loss: 0.1349 - accuracy: 0.94 - ETA: 8s - loss: 0.1269 - accuracy: 0.95 - ETA: 8s - loss: 0.1282 - accuracy: 0.95 - ETA: 8s - loss: 0.1381 - accuracy: 0.94 - ETA: 7s - loss: 0.1504 - accuracy: 0.94 - ETA: 7s - loss: 0.1524 - accuracy: 0.93 - ETA: 7s - loss: 0.1495 - accuracy: 0.93 - ETA: 7s - loss: 0.1439 - accuracy: 0.94 - ETA: 7s - loss: 0.1417 - accuracy: 0.94 - ETA: 6s - loss: 0.1371 - accuracy: 0.94 - ETA: 6s - loss: 0.1418 - accuracy: 0.94 - ETA: 6s - loss: 0.1415 - accuracy: 0.94 - ETA: 6s - loss: 0.1400 - accuracy: 0.94 - ETA: 6s - loss: 0.1375 - accuracy: 0.94 - ETA: 5s - loss: 0.1387 - accuracy: 0.94 - ETA: 5s - loss: 0.1405 - accuracy: 0.94 - ETA: 5s - loss: 0.1380 - accuracy: 0.94 - ETA: 5s - loss: 0.1360 - accuracy: 0.94 - ETA: 5s - loss: 0.1349 - accuracy: 0.94 - ETA: 4s - loss: 0.1397 - accuracy: 0.94 - ETA: 4s - loss: 0.1373 - accuracy: 0.94 - ETA: 4s - loss: 0.1407 - accuracy: 0.94 - ETA: 4s - loss: 0.1404 - accuracy: 0.94 - ETA: 4s - loss: 0.1399 - accuracy: 0.94 - ETA: 3s - loss: 0.1391 - accuracy: 0.94 - ETA: 3s - loss: 0.1371 - accuracy: 0.94 - ETA: 3s - loss: 0.1421 - accuracy: 0.94 - ETA: 3s - loss: 0.1448 - accuracy: 0.94 - ETA: 2s - loss: 0.1427 - accuracy: 0.94 - ETA: 2s - loss: 0.1398 - accuracy: 0.94 - ETA: 2s - loss: 0.1399 - accuracy: 0.94 - ETA: 2s - loss: 0.1377 - accuracy: 0.94 - ETA: 2s - loss: 0.1374 - accuracy: 0.94 - ETA: 1s - loss: 0.1383 - accuracy: 0.94 - ETA: 1s - loss: 0.1371 - accuracy: 0.94 - ETA: 1s - loss: 0.1376 - accuracy: 0.94 - ETA: 1s - loss: 0.1409 - accuracy: 0.94 - ETA: 1s - loss: 0.1426 - accuracy: 0.94 - ETA: 0s - loss: 0.1472 - accuracy: 0.94 - ETA: 0s - loss: 0.1456 - accuracy: 0.94 - ETA: 0s - loss: 0.1444 - accuracy: 0.94 - ETA: 0s - loss: 0.1444 - accuracy: 0.94 - 12s 7ms/sample - loss: 0.1438 - accuracy: 0.9458 - val_loss: 0.2543 - val_accuracy: 0.8955\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x22916202c08>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, Y, batch_size=32, epochs=5,validation_split=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No model found in config file.",
     "output_type": "error",
     
    }
   ],
   "source": [
   
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
