{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullyConnectedLayer(object):\n",
    "    \n",
    "    def __init__(self, num_inputs , layer_size , activation_function, derivated_activation_function = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = np.random.standard_normal((num_inputs , layer_size))\n",
    "        self.b = np.random.standard_normal(layer_size)\n",
    "        self.size = layer_size\n",
    "        \n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "        self.derivated_activation_function = derivated_activation_function\n",
    "        self.H , self.out_put = None , None\n",
    "        \n",
    "        self.dLoss_dW , self.dLoss_db = None , None\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward( self, H_input ):\n",
    "        z = np.dot(H_input , self.W ) + self.b\n",
    "        self.out_put = self.activation_function(z)\n",
    "        self.H =  H_inputput\n",
    "        return self.out_put\n",
    "    \n",
    "    def backward(self, dLoss_dOut):\n",
    "        dOut_dOin = self.derivated_activation_function(self.out_put)\n",
    "        dLoss_dOin = (dLoss_dOut *  dOut_dOin)\n",
    "        \n",
    "        dOin_dW = self.H.T\n",
    "        dOin_db = np.ones(dLoss_dOut.shape[0])\n",
    "        dOin_dH = self.W.T\n",
    "        \n",
    "        \n",
    "        self.dLoss_dW = np.dot(dOin_dW , dLoss_dOin )\n",
    "        self.dLoss_db = np.dot(dOin_db, dLoss_dOin)\n",
    "        \n",
    "        dLoss_dH  = np.dot(dLoss_dOin , dOin_dH )\n",
    "        \n",
    "        return dL_dH\n",
    "    \n",
    "    def optimize(self, epsilon ):\n",
    "        self.W = self.W  - epsilon*self.dL_dW\n",
    "        self.b = self.b  - epsilon*self.dL_db\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### derivated_sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivated_sigmoid(y):\n",
    "    return y*(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 loss function \n",
    "def loss_L2(pred , target ):\n",
    "    return np.sum(np.square(pred - target))/pred.shape[0]\n",
    "    # divided by batch_size buz of take averge\n",
    "    \n",
    "    \n",
    "def derivated_loss_L2(pred , target) :\n",
    "    return 2*(pred - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred , target ):\n",
    "    y = np.multiply(np.log(pred) , target) + np.multiply(np.log(1-pred),(1-target))\n",
    "    return -np.mean(y)\n",
    "\n",
    "\n",
    "\n",
    "def derivated_cross_entropy(pred , target ):\n",
    "    return (pred - target )/ (pred*(1-pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork:\n",
    "    \n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32),\n",
    "                 activation_function=sigmoid, derivated_activation_function=derivated_sigmoid,\n",
    "                 loss_function=loss_L2, derivated_loss_function=derivated_loss_L2):\n",
    "        layer_sizes = [num_inputs , *hidden_layers_sizes , num_outputs]\n",
    "        self.layers = [FullyConnectedLayer(layer_sizes[i] , layer_sizes[i+1], activation_function , derivated_activation_function) for i in range(len(layer_sizes)-1)]\n",
    "        \n",
    "        self.loss_function = loss_function\n",
    "        self.derivated_loss_function = derivated_loss_function\n",
    "        \n",
    "    def forward(self , H ):\n",
    "        for layer in self.layers:\n",
    "            x = layer.forward(H)\n",
    "        return x \n",
    "    \n",
    "    \n",
    "    def backward(self, dLoss_dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dLoss_dout = layer.backward(dLoss_dout)\n",
    "        return dLoss_dout\n",
    "    \n",
    "    def optimize(self, alpa ):\n",
    "        for layer in self.layers:\n",
    "            layer.optimize(alpa)\n",
    "            \n",
    "    def predict( self, input_x ):\n",
    "        estimations_value = self.forward(input_x)\n",
    "        best_class = np.argmax(estimations_value)\n",
    "        return best_class\n",
    "    \n",
    "    \n",
    "    def evaluate_accuracy( self, X_val , y_val ):\n",
    "        num_corrects = 0\n",
    "        for i in range(len(X_val)):\n",
    "            pred_class = self.predict(X_val[i])\n",
    "            if pred_class  == y_val[i]:\n",
    "                num_corrects += 1\n",
    "        return num_corrects/len(X_val)\n",
    "    \n",
    "    def train(self, X_train , y_train , X_val = None , y_val = None , \n",
    "             batch_size  = 32 , num_epochs = 5 , learning_rate = 1e-3 , print_frequency = 20  ):\n",
    "        \n",
    "        num_batches_per_epoch  = len(X_train)//batch_size\n",
    "        do_validation = X_val is not None and y_val is not None\n",
    "        \n",
    "        losses , accuracies = [] ,[]\n",
    "        for i in range(num_epochs):\n",
    "            epoch_loss  = 0\n",
    "            for b in range(num_batches_per_epoch):\n",
    "                \n",
    "                batch_index_begin = b*batch_size\n",
    "                batch_index_end = 2*batch_index_begin\n",
    "                \n",
    "                x = X_train[batch_index_begin : batch_index_end]\n",
    "                targets = y_train[batch_index_begin : batch_index_end]\n",
    "                \n",
    "                #Optimize  on batch \n",
    "                predictions = y = self.forward(x)\n",
    "                loss = self.loss_function(predictions, targets )\n",
    "                \n",
    "                dLoss_dOut = self.derivated_loss_function(predictions, targets)\n",
    "                self.backward(dLoss_dOut)\n",
    "                self.optimize(learning_rate)\n",
    "                epoch_loss += loss\n",
    "                \n",
    "            epoch_loss /= num_batches_per_epoch\n",
    "            loss.append(epoch_loss)\n",
    "            if do_validation:\n",
    "                accuracy = self.evaluate_accuracy(X_val, y_val)\n",
    "                accuracies.append(accuracy)\n",
    "            else:\n",
    "                accuracy = np.NaN\n",
    "                \n",
    "            if i % print_frequency == 0 or i == (num_epochs - 1):\n",
    "                print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(i, epoch_loss, accuracy * 100))\n",
    "        return losses, accuracies\n",
    "                \n",
    "        \n",
    "\n",
    "    \n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
