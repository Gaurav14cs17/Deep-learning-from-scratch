{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class FullyConnectedLayer(object):\n",
    "    \n",
    "    def __init__(self, num_inputs , layer_size , activation_function, derivated_activation_function = None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.W = np.random.standard_normal((num_inputs, layer_size))\n",
    "        self.b = np.random.standard_normal(layer_size)\n",
    "        self.size = layer_size\n",
    "        \n",
    "        \n",
    "        self.activation_function = activation_function\n",
    "        self.derivated_activation_function = derivated_activation_function\n",
    "        self.H , self.out_put = None , None\n",
    "        \n",
    "        self.dLoss_dW , self.dLoss_db = None , None\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward( self, H_input ):\n",
    "        z = np.dot(H_input , self.W ) + self.b\n",
    "        self.out_put = self.activation_function(z)\n",
    "        self.H =  H_input\n",
    "        return self.out_put\n",
    "    \n",
    "    def backward(self, dLoss_dOut):\n",
    "        dOut_dOin = self.derivated_activation_function(self.out_put)\n",
    "        dLoss_dOin = (dLoss_dOut *  dOut_dOin)\n",
    "        \n",
    "        dOin_dW = self.H.T\n",
    "        dOin_db = np.ones(dLoss_dOut.shape[0])\n",
    "        dOin_dH = self.W.T\n",
    "        \n",
    "        \n",
    "        self.dLoss_dW = np.dot(dOin_dW , dLoss_dOin )\n",
    "        self.dLoss_db = np.dot(dOin_db, dLoss_dOin)\n",
    "        \n",
    "        dLoss_dH  = np.dot(dLoss_dOin , dOin_dH )\n",
    "        \n",
    "        return dLoss_dH\n",
    "    \n",
    "    def optimize(self, epsilon ):\n",
    "        self.W = self.W  - epsilon*self.dLoss_dW\n",
    "        self.b = self.b  - epsilon*self.dLoss_db\n",
    "        \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / ( 1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### derivated_sigmoid(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivated_sigmoid(y):\n",
    "    return y*(1-y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L2 loss function \n",
    "def loss_L2(pred , target ):\n",
    "    return np.sum(np.square(pred - target))/pred.shape[0]\n",
    "    # divided by batch_size buz of take averge\n",
    "    \n",
    "    \n",
    "def derivated_loss_L2(pred , target) :\n",
    "    return 2*(pred - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(pred , target ):\n",
    "    y = np.multiply(np.log(pred) , target) + np.multiply(np.log(1-pred),(1-target))\n",
    "    return -np.mean(y)\n",
    "\n",
    "\n",
    "\n",
    "def derivated_cross_entropy(pred , target ):\n",
    "    return (pred - target )/ (pred*(1-pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork:\n",
    "    \n",
    "    def __init__(self, num_inputs, num_outputs, hidden_layers_sizes=(64, 32 , 16),\n",
    "                 activation_function=sigmoid, derivated_activation_function=derivated_sigmoid,\n",
    "                 loss_function=loss_L2, derivated_loss_function=derivated_loss_L2):\n",
    "        super().__init__()\n",
    "        layer_sizes = [num_inputs, *hidden_layers_sizes, num_outputs]\n",
    "        print(\"All layer node in FCN\")\n",
    "        print(layer_sizes)\n",
    "        self.layers = [\n",
    "            FullyConnectedLayer(layer_sizes[i], layer_sizes[i + 1], activation_function, derivated_activation_function)\n",
    "            for i in range(len(layer_sizes) - 1)]\n",
    "\n",
    "        self.loss_function = loss_function\n",
    "        self.derivated_loss_function = derivated_loss_function\n",
    "        \n",
    "    def forward(self , H ):\n",
    "        for layer in self.layers:\n",
    "            H = layer.forward(H)\n",
    "        return H \n",
    "    \n",
    "    \n",
    "    def backward(self, dLoss_dout):\n",
    "        for layer in reversed(self.layers):\n",
    "            dLoss_dout = layer.backward(dLoss_dout)\n",
    "        return dLoss_dout\n",
    "    \n",
    "    def optimize(self, alpa ):\n",
    "        for layer in self.layers:\n",
    "            layer.optimize(alpa)\n",
    "            \n",
    "    def predict( self, input_x ):\n",
    "        estimations_value = self.forward(input_x)\n",
    "        best_class = np.argmax(estimations_value)\n",
    "        return best_class\n",
    "    \n",
    "    \n",
    "    def evaluate_accuracy( self, X_val , y_val ):\n",
    "        num_corrects = 0\n",
    "        for i in range(len(X_val)):\n",
    "            pred_class = self.predict(X_val[i])\n",
    "            if pred_class  == y_val[i]:\n",
    "                num_corrects += 1\n",
    "        return num_corrects/len(X_val)\n",
    "    \n",
    "    def train(self, X_train, y_train, X_val=None, y_val=None, \n",
    "              batch_size=32, num_epochs=5, learning_rate=1e-3, print_frequency=20):\n",
    "      \n",
    "        num_batches_per_epoch  = len(X_train)//batch_size\n",
    "        do_validation = X_val is not None and y_val is not None\n",
    "        losses, accuracies = [], []\n",
    "        \n",
    "        for i in range(num_epochs):\n",
    "            epoch_loss  = 0\n",
    "            for b in range(num_batches_per_epoch):\n",
    "                \n",
    "                batch_index_begin = b*batch_size\n",
    "                batch_index_end =  batch_index_begin +  batch_size\n",
    "                \n",
    "                x = X_train[batch_index_begin : batch_index_end]\n",
    "                targets = y_train[batch_index_begin : batch_index_end]\n",
    "                \n",
    "                #Optimize  on batch \n",
    "                predictions = y = self.forward(x)\n",
    "                loss = self.loss_function(predictions, targets )\n",
    "                \n",
    "                dLoss_dOut = self.derivated_loss_function(predictions, targets)\n",
    "                self.backward(dLoss_dOut)\n",
    "                self.optimize(learning_rate)\n",
    "                epoch_loss += loss\n",
    "                \n",
    "            epoch_loss /= num_batches_per_epoch\n",
    "            losses.append(epoch_loss)\n",
    "            if do_validation:\n",
    "                accuracy = self.evaluate_accuracy(X_val, y_val)\n",
    "                accuracies.append(accuracy)\n",
    "            else:\n",
    "                accuracy = np.NaN\n",
    "                \n",
    "            if i % print_frequency == 0 or i == (num_epochs - 1):\n",
    "                print(\"Epoch {:4d}: training loss = {:.6f} | val accuracy = {:.2f}%\".format(i, epoch_loss, accuracy * 100))\n",
    "        return losses, accuracies\n",
    "                \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# !pip install matplotlib  # Uncomment and run if matplotlib is not installed yet.\n",
    "import matplotlib          # We use this package to visualize some data and results\n",
    "import matplotlib.pyplot as plt\n",
    "import mnist\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = mnist.train_images(), mnist.train_labels()\n",
    "X_test,  y_test  = mnist.test_images(), mnist.test_labels()\n",
    "num_classes = 10    # classes are the digits from 0 to 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOcAAADnCAYAAADl9EEgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAEH0lEQVR4nO3dsS5sCxSA4Ts3xyR0GiJRaHQSWj2d59R4Dc20iI6aTiYaCnNfgD0nBvPP9X2llb2zmz8rOSuO0Ww2+wfo+XfZHwC8T5wQJU6IEidEiROi/syZ+6dc+H6j935oc0KUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6Lm/QlAVszBwcHg/PX1dXB+cXHx4ezw8PBT38Tn2JwQJU6IEidEiROixAlR4oQocUKUO+eKubm5GZzf3t4Ozmez2eD84eHhw5k758+yOSFKnBAlTogSJ0SJE6LECVHihCh3zhUz7045b87qsDkhSpwQJU6IEidEiROixAlRTikr5vr6etmfwA+xOSFKnBAlTogSJ0SJE6LECVHihCh3zhXjzvl72JwQJU6IEidEiROixAlR4oQocUKUO+eKubu7W/Yn8ENsTogSJ0SJE6LECVHihChxQpQ4Icqdc8VMJpOFnh+Px4Pz7e3thd7P17E5IUqcECVOiBInRIkTosQJUeKEKHfOX2ZjY2NwfnR09ENfwjw2J0SJE6LECVHihChxQpQ4IcopJWY6nQ7OX15eFnr/1tbWQs/zc2xOiBInRIkTosQJUeKEKHFClDghyp0z5urqanD++Pi40PtPT08Xep6fY3NClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVF+n/OX2dvbW/Yn8JdsTogSJ0SJE6LECVHihChxQpQ4Icqd85cZjUbL/gT+ks0JUeKEKHFClDghSpwQJU6IckqJOT8//9b3Hx8ff+v7+To2J0SJE6LECVHihChxQpQ4IUqcEOXOGTOZTJb9CUTYnBAlTogSJ0SJE6LECVHihChxQtRoNpsNzQeHfL3Nzc3B+dPT0+B8d3d3cH5/fz84X1tbG5zzLd79/0ptTogSJ0SJE6LECVHihChxQpQ4Icrvc/7PzLuTumOuDpsTosQJUeKEKHFClDghSpwQ5ZSyBJeXlx/OptPpQu8+Oztb6Hk6bE6IEidEiROixAlR4oQocUKUOCHKnXMJnp+fP5y9vb0t9O6Tk5OFnqfD5oQocUKUOCFKnBAlTogSJ0SJE6LcOZdgZ2fnw9l4PB58dn19fXC+v7//qW+ix+aEKHFClDghSpwQJU6IEidEiROiRrPZbGg+OAS+xOi9H9qcECVOiBInRIkTosQJUeKEKHFClDghSpwQJU6IEidEiROixAlR4oQocUKUOCFKnBAlTogSJ0SJE6LECVHihChxQpQ4IUqcECVOiBInRIkTosQJUeKEqD9z5u/+aTLg+9mcECVOiBInRIkTosQJUeKEqP8AlI5VPk2EYDcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img_idx = np.random.randint( 0 , X_test.shape[0])\n",
    "plt.imshow(X_test[img_idx] , cmap = matplotlib.cm.binary)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(y_test[img_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(-1, 28 * 28), X_test.reshape(-1, 28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized pixel values between 0.0 and 1.0\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test = X_train / 255., X_test / 255.\n",
    "print(\"Normalized pixel values between {} and {}\".format(X_train.min(), X_train.max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.eye(num_classes)[y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 1., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layer node in FCN\n",
      "[784, 64, 32, 16, 10]\n"
     ]
    }
   ],
   "source": [
    "mnist_classifier = SimpleNetwork(num_inputs=X_train.shape[1], \n",
    "                                 num_outputs=num_classes, hidden_layers_sizes=[64, 32, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained : training loss = 3.784788 | val accuracy = 9.78%\n"
     ]
    }
   ],
   "source": [
    "predictions = mnist_classifier.forward(X_train)                         # forward pass\n",
    "loss_untrained = mnist_classifier.loss_function(predictions, y_train)   # loss computation\n",
    "\n",
    "accuracy_untrained = mnist_classifier.evaluate_accuracy(X_test, y_test)  # Accuracy\n",
    "print(\"Untrained : training loss = {:.6f} | val accuracy = {:.2f}%\".format(loss_untrained, accuracy_untrained * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, accuracies = mnist_classifier.train(X_train, y_train, X_test, y_test, \n",
    "                                            batch_size=30, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
