{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "img_rows , img_cols , img_ch = 28 , 28 ,1 \n",
    "input_shape = (img_rows , img_cols , img_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train , y_train ) ,(x_test , y_test )  = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train/ 255.0\n",
    "x_test  = x_test / 255.0\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(x_train.shape[0], *input_shape)\n",
    "x_test = x_test.reshape(x_test.shape[0], *input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import (Input, Activation, Dense, Flatten, Conv2D, \n",
    "                                     MaxPooling2D, Dropout, BatchNormalization)\n",
    "\n",
    "epochs      = 200\n",
    "batch_size  = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def con_layer(x , kernels , bias , s ):\n",
    "    z = tf.nn.conv2d( x , kernels , strides=[1 , s , s, 1] , padding='VALID')\n",
    "    z = tf.nn.relu(z + bias )\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv_layer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, num_kernals = 32 , kernal_size = (3 , 3 ) , stride= 1):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.num_kernals = num_kernals\n",
    "        self.kernal_size = kernal_size\n",
    "        self.stride = stride\n",
    "        \n",
    "    def build( self, inputs ):\n",
    "        num_channels = inputs[-1]\n",
    "        kernals_shape = (*self.kernal_size , num_channels  , self.num_kernals)\n",
    "        \n",
    "        glorot_init = tf.initializers.GlorotUniform()\n",
    "        self.kernels = self.add_weight(name= 'kernal',\n",
    "                                      shape = kernals_shape ,\n",
    "                                      initializer = glorot_init ,\n",
    "                                      trainable = True)\n",
    "        self.bias = self.add_weight(name = 'bias',\n",
    "                                   shape = (self.num_kernals,),\n",
    "                                   initializer = 'random_normal',\n",
    "                                   trainable = True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        return con_layer(inputs , self.kernels , self.bias , self.stride)\n",
    "    \n",
    "    def get_config(self):\n",
    "        {\n",
    "          'kernel_size': self.kernel_size,\n",
    "            'strides': self.strides,\n",
    "            'use_bias': self.use_bias\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "def l2_reg(coef=1e-2):\n",
    "    \"\"\"\n",
    "    Returns a function computing a weighed L2 norm of a given tensor.\n",
    "    (this is basically a reimplementation of f.keras.regularizers.l2())\n",
    "    :param coef:    Weight for the norm\n",
    "    :return:        Loss function\n",
    "    \"\"\"\n",
    "    return lambda x: tf.reduce_sum(x ** 2) * coef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvWithRegularizers(SimpleConv_layer):\n",
    "    \n",
    "    def __init__(self, num_kernels = 32 , \n",
    "                kernel_size = (3 ,3 ) ,\n",
    "                stride =  1, \n",
    "                kernel_regularizer = l2_reg(),\n",
    "                bias_regularizer = None ):\n",
    "        super().__init__(num_kernels , kernel_size , stride)\n",
    "        self.kernel_regularizer = kernel_regularizer\n",
    "        self.bias_regularizer = bias_regularizer\n",
    "        \n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super().build(input_shape)\n",
    "        if self.kernel_regularizer is not None:\n",
    "            self.add_loss(partial(self.kernel_regularizer, self.kernels))\n",
    "        if self.bias_regularizer is not None:\n",
    "            self.add_loss(partial(self.bias_regularizer, self.bias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = ConvWithRegularizers(num_kernels=32, kernel_size=(3, 3), stride=1,\n",
    "                            kernel_regularizer=l2_reg(1.), bias_regularizer=l2_reg(1.))\n",
    "conv.build(input_shape=tf.TensorShape((None, 28, 28, 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor: id=32, shape=(), dtype=float32, numpy=1.9072797>, <tf.Tensor: id=39, shape=(), dtype=float32, numpy=0.050915994>]\n"
     ]
    }
   ],
   "source": [
    "print(conv.losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(Model):\n",
    "    \n",
    "    def __init__(self, num_classes , kernel_regularizer=l2_reg(), bias_regularizer=l2_reg()):\n",
    "        super(LeNet5 , self ).__init__()\n",
    "        \n",
    "        self.conv1 = ConvWithRegularizers(\n",
    "        6 , kernel_size = (5 ,5 ) , kernel_regularizer=kernel_regularizer , bias_regularizer=bias_regularizer)\n",
    "        \n",
    "        self.conv2 = ConvWithRegularizers(\n",
    "        32 , kernel_size = (5,5) , kernel_regularizer=kernel_regularizer , bias_regularizer=bias_regularizer)\n",
    "        \n",
    "        \n",
    "        self.max_pool = tf.keras.layers.MaxPool2D(pool_size=( 2,2 ))\n",
    "        self.flatten  = tf.keras.layers.Flatten()\n",
    "        \n",
    "        self.hidden_layer1 = tf.keras.layers.Dense(120 , activation='relu')\n",
    "        self.hidden_layer2 = tf.keras.layers.Dense(84 , activation='relu')\n",
    "        self.out_put_layer = tf.keras.layers.Dense(num_classes , activation='softmax')\n",
    "        \n",
    "    def call(self, x ):\n",
    "        x = self.conv1(x)\n",
    "        x = self.max_pool(x)\n",
    "        \n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x= self.max_pool(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.hidden_layer1(x)\n",
    "        x = self.hidden_layer2(x)\n",
    "        return self.out_put_layer(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.SGD()\n",
    "model = LeNet5(10, kernel_regularizer=l2_reg(), bias_regularizer=l2_reg())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset    = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 3753 calls to <function con_layer at 0x000001DD40106F78> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings is likely due to passing python objects instead of tensors. Also, tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. Please refer to https://www.tensorflow.org/beta/tutorials/eager/tf_function#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for more details.\n"
     ]
    }
   ],
   "source": [
    "batched_input_shape = tf.TensorShape((None, *input_shape))\n",
    "model.build(input_shape=batched_input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"le_net5_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv_with_regularizers_3 (Co multiple                  156       \n",
      "_________________________________________________________________\n",
      "conv_with_regularizers_4 (Co multiple                  4832      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 multiple                  0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          multiple                  0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  61560     \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  10164     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              multiple                  850       \n",
      "=================================================================\n",
      "Total params: 77,562\n",
      "Trainable params: 77,562\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer le_net5_2 is casting an input tensor from dtype float64 to the layer's dtype of float32, which is new behavior in TensorFlow 2.  The layer has dtype float32 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float32, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float64 by default, call `tf.keras.backend.set_floatx('float64')`. To change just this layer, pass dtype='float64' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "log_string_template = 'Epoch {0:3}/{1}: main loss = {5}{2:5.3f}{6}; ' + \\\n",
    "                      'reg loss = {5}{3:5.3f}{6}; val acc = {5}{4:5.3f}%{6}'\n",
    "\n",
    "def train_classifier_on_mnist(model, log_frequency=10):\n",
    "\n",
    "    avg_main_loss = tf.keras.metrics.Mean(name='avg_main_loss', dtype=tf.float32)\n",
    "    avg_reg_loss  = tf.keras.metrics.Mean(name='avg_reg_loss', dtype=tf.float32)\n",
    "\n",
    "    #print(\"Training: {}start{}\".format(log_begin_red, log_end_format))\n",
    "    for epoch in range(epochs):\n",
    "        for (batch_images, batch_gts) in dataset:    # For each batch of this epoch\n",
    "\n",
    "            with tf.GradientTape() as grad_tape:     # Tell TF to tape the gradients\n",
    "                y = model(batch_images)              # Feed forward\n",
    "                main_loss = tf.losses.sparse_categorical_crossentropy(\n",
    "                    batch_gts, y)                    # Compute loss\n",
    "                reg_loss = sum(model.losses)         # List and add other losses\n",
    "                loss = main_loss + reg_loss\n",
    "\n",
    "            # Get the gradients of combined losses and back-propagate:\n",
    "            grads = grad_tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # Keep track of losses for display:\n",
    "            avg_main_loss.update_state(main_loss)\n",
    "            avg_reg_loss.update_state(reg_loss)\n",
    "\n",
    "        if epoch % log_frequency == 0 or epoch == (epochs - 1): # Log some metrics\n",
    "            # Validate, computing the accuracy on test data:\n",
    "            acc = tf.reduce_mean(tf.metrics.sparse_categorical_accuracy(\n",
    "                tf.constant(y_test), model(x_test))).numpy() * 100\n",
    "\n",
    "            main_loss = avg_main_loss.result()\n",
    "            reg_loss = avg_reg_loss.result()\n",
    "\n",
    "           # print(log_string_template.format(\n",
    "                #epoch, epochs, main_loss, reg_loss, acc, log_begin_blue, log_end_format))\n",
    "\n",
    "        avg_main_loss.reset_states()\n",
    "        avg_reg_loss.reset_states()\n",
    "    print(\"Training: {}end{}\".format(log_begin_green, log_end_format))\n",
    "    return model\n",
    "    \n",
    "\n",
    "model = LeNet5(10, kernel_regularizer=l2_reg(), bias_regularizer=l2_reg())\n",
    "model = train_classifier_on_mnist(model, log_frequency=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
